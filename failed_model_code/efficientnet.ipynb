{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from barbar import Bar\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "input_root = './shopee-product-detection-dataset'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(input_root, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(input_root, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "class ShopeeTestDataset(Dataset):\n",
    "    def __init__(self, train_file_lt, transform=None):\n",
    "        self.imgs = train_file_lt\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.imgs[idx]\n",
    "        img = Image.open(os.path.join(input_root, 'test', 'test', img_filename))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        filename = self.imgs[idx]\n",
    "        return img, filename\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # https://discuss.pytorch.org/t/randomresizedcrop-vs-resize-updated/35357/2\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "test_filenames = test_df['filename'].tolist()\n",
    "image_datasets = {'train': datasets.ImageFolder(os.path.join(input_root, 'train/train'),\n",
    "                                          data_transforms['train']),\n",
    "                  'test': ShopeeTestDataset(test_filenames, transform=data_transforms['test'])}\n",
    "dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=256,\n",
    "                                              shuffle=True, num_workers=4),\n",
    "               'test': torch.utils.data.DataLoader(image_datasets['test'], batch_size=512,\n",
    "                                              shuffle=False, num_workers=0)}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b1\n"
     ]
    }
   ],
   "source": [
    "model_ft = EfficientNet.from_pretrained('efficientnet-b1', num_classes=42) # https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "# True Unfreeze model weights https://www.kaggle.com/ateplyuk/pytorch-efficientnet\n",
    "# for param in model_ft.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# num_ftrs = model_ft._fc.in_features\n",
    "# Here the size of each output sample is set to 42.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "# model_ft._fc = nn.Linear(num_ftrs, 42)\n",
    "\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 7.3ssss\n",
      "train Loss: 4.3647 Acc: 0.0460\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 4.0ssss\n",
      "train Loss: 3.4349 Acc: 0.0961\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 4.0ssss\n",
      "train Loss: 3.2213 Acc: 0.1433\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.9ssss\n",
      "train Loss: 3.0716 Acc: 0.1779\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 4.1ssss\n",
      "train Loss: 2.9225 Acc: 0.2140\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 4.0ssss\n",
      "train Loss: 2.6141 Acc: 0.2911\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 4.0ssss\n",
      "train Loss: 2.5316 Acc: 0.3125\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 4.0ssss\n",
      "train Loss: 2.4708 Acc: 0.3284\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.8ssss\n",
      "train Loss: 2.4184 Acc: 0.3414\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 4.0ssss\n",
      "train Loss: 2.3643 Acc: 0.3558\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.9ssss\n",
      "train Loss: 2.2669 Acc: 0.3820\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.8ssss\n",
      "train Loss: 2.2394 Acc: 0.3884\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.9ssss\n",
      "train Loss: 2.2228 Acc: 0.3922\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.9ssss\n",
      "train Loss: 2.2095 Acc: 0.3943\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 4.1ssss\n",
      "train Loss: 2.1959 Acc: 0.4002\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.8ssss\n",
      "train Loss: 2.1810 Acc: 0.4037\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.9ssss\n",
      "train Loss: 2.1774 Acc: 0.4052\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.9ssss\n",
      "train Loss: 2.1742 Acc: 0.4068\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.9ssss\n",
      "train Loss: 2.1750 Acc: 0.4059\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.9ssss\n",
      "train Loss: 2.1748 Acc: 0.4060\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.9ssss\n",
      "train Loss: 2.1686 Acc: 0.4075\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.8ssss\n",
      "train Loss: 2.1696 Acc: 0.4070\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.8ssss\n",
      "train Loss: 2.1696 Acc: 0.4081\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.9ssss\n",
      "train Loss: 2.1666 Acc: 0.4071\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "105392/105392: [===============================>] - ETA 3.8ssss\n",
      "train Loss: 2.1689 Acc: 0.4062\n",
      "\n",
      "Training complete in 216m 44s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (inputs, labels) in enumerate(Bar(dataloaders[phase])):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-1)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 5 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft.load_state_dict(torch.load('efficientb0.pth'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "torch.save(model_ft.state_dict(), 'efficientb1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/2341#issuecomment-442759206\n",
    "def submission(model):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, filenames) in enumerate(dataloaders['test']):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            # https://discuss.pytorch.org/t/get-value-out-of-torch-cuda-float-tensor/2539/7\n",
    "            # print(preds.data.tolist())\n",
    "            # print(filenames)\n",
    "            # print(preds)\n",
    "            results.extend(preds.data.tolist())\n",
    "            \n",
    "        model.train(mode=was_training)\n",
    "    \n",
    "    df = pd.DataFrame(data={\n",
    "        'filename': test_filenames,\n",
    "        'category': results,\n",
    "    })\n",
    "    df['category'] = df['category'].apply(lambda x: str(x).zfill(2))\n",
    "    df.to_csv(os.path.join('./', 'efficientb1.csv'), index=False)\n",
    "\n",
    "submission(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
